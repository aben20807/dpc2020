# Deep Learning System and Parallel Computing HW2

+ Author: 黃柏瑄 (P78081528)

## Experiment description

+ Framework: Chainer
+ Dataset: Cifar10
+ Model: ResNet20

Profile ResNet20 inference for Cifar10 dataset on Chainer without GPU.

## Environment setting

+ Hardware:
  + CPU: Intel(R) Core(TM) i7-10700K CPU @ 3.80GHz (`$ lscpu | grep 'Model name'`)
  + Memory: 32 GB
+ Software:
  + OS: Ubuntu 18.04.5 LTS (`$ lsb_release -d`)
  + Nvdia GPU driver 455.23.05 (`$ nvidia-smi --query | grep 'Driver Version'`)
  + CUDA 10.2 (`$ nvidia-smi --query | grep 'CUDA Version'`)
  + Python 3.6.9
  + Chainer 7.7.0

```bash
$ ubuntu-drivers devices
$ sudo apt install nvidia-driver-455
$ sudo reboot
$ nvidia-smi # Check the installation of Nvidia driver

$ wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin
$ sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600
$ wget https://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01_1.0-1_amd64.deb
$ sudo dpkg -i cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01_1.0-1_amd64.deb
$ sudo apt-key add /var/cuda-repo-10-2-local-10.2.89-440.33.01/7fa2af80.pub
$ sudo apt-get update
$ sudo apt-get -y install cuda
$ printf '\nexport CUDA_HOME=/usr/local/cuda-10.2' >> ~/.bashrc
$ printf '\nexport LD_LIBRARY_PATH=${CUDA_HOME}/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
$ printf '\nexport PATH=$PATH:$CUDA_HOME/bin' >> ~/.bashrc
$ source ~/.bashrc
$ nvcc -V  # Check the installation of CUDA
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Wed_Oct_23_19:24:38_PDT_2019
Cuda compilation tools, release 10.2, V10.2.89
$ sudo reboot # Solve "Failed to initialize NVML: Driver/library version mismatch" of nvidia-smi

$ tar -xzvf cudnn-10.2-linux-x64-v8.0.5.39.tgz
$ sudo cp cuda/include/cudnn*.h /usr/local/cuda/include
$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64
$ sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*

```



```bash
$ virtualenv --python python3.6 env
$ source env/bin/activate
$ pip install chainer==7.7.0
$ pip install 'cupy-cuda102>=7.7.0,<8.0.0'
$ pip install tqdm
$ pip list
Package           Version
----------------- -------
chainer           7.7.0
filelock          3.0.12
numpy             1.19.4
pip               20.2.4
protobuf          3.14.0
setuptools        50.3.2
six               1.15.0
tqdm              4.54.1
typing-extensions 3.7.4.3
wheel             0.35.1
```

<div style="page-break-after: always; break-after: page;"></div>

## Methodology

### `hook.print_report()`

This function will print a pretty report but the time are fused by the type. We cannot figure out which part caused more time but only the type.

<img src="images/P78081528_HW2/Screenshot from 2020-11-16 14-04-42.png" alt="Screenshot from 2020-11-16 14-04-42" style="zoom: 50%;" />

### `hook.total_time()`

I found that `TimerHook` accumulated the result, so it could not be done by pseudo code (List 1) to profile the model.

```python
# Layer1
print("#Layer1")
hook = TimerHook()
with hook:
    h = F.max_pooling_2d(F.local_response_normalization(F.relu(self.conv1(x))), 3, stride=2)
	print("total time:", hook.total_time(), "s") # Layer1's executioin time
# Layer2
print("#Layer2")
hook = TimerHook()
with hook:
    h = F.max_pooling_2d(F.local_response_normalization(F.relu(self.conv1(x))), 3, stride=2)
	print("total time:", hook.total_time(), "s") # Layer2's executioin time
```

> List 1: Layer2's executioin time would include Layer1's time

### `hook.call_history`

To handle all layer once, I used `call_history`, which provided by `TimerHook`. The `call_history` variable contained layer-wise record with the layer type and the execution time. Therefore, List 2 showed the workable version.

```python
hook = TimerHook()
with hook:
    y = model(x)
    
import pprint
pprint.pprint(hook.call_history) # print the layer-wise result
```

> List 2: use `hook.call_history`

<div style="page-break-after: always; break-after: page;"></div>

#### Difference with hw1

+ hw1's inference.py:
```python
	y = model(x)  # Inference result
```

+ hw2's profile.py:
```python
	trials = 10000
    num_layer = 0  # For counting the number of layers of the model
    hook = TimerHook()
    y = []
    with hook:
        for i in range(trials):
            y = model(x)  # Inference result
            if i == 0:
                num_layer = len(hook.call_history)

    result = {}
    for i in range(len(hook.call_history)):
        layer_i = i % num_layer
        if not layer_i in result.keys():
            result[layer_i] = {
                "type": hook.call_history[i][0],
                "time": hook.call_history[i][1],
            }
        else:
            result[layer_i]["time"] += hook.call_history[i][1]
        i += 1

    average_time = dict(
        map(lambda kv: (kv[0], (kv[1]["time"] / trials)), result.items())
    ) # Calculate the average time
    for k in result.keys():
        print(f'{k}, {result[k]["type"]}, {average_time[k]}')
```

After 10,000 inferences, `call_history` will contain 10,000 * 66 records (66 is the number of the layers of ResNet20). To get the average result, I accumulated the time of the same index, i.e., 0, 66, 132... will summed in 0. The final result showed in section 4.

![Screenshot from 2020-11-16 11-49-23](images/P78081528_HW2/Screenshot%20from%202020-11-16%2011-49-23.png)

> Execution progress

<div style="page-break-after: always; break-after: page;"></div>

## Result

![Layer-wise execution time of one inference of ResNet20 for Cifar10 (Averaged by 10,000 inferences)](images/P78081528_HW2/Layer-wise%20execution%20time%20of%20one%20inference%20of%20ResNet20%20for%20Cifar10%20(Averaged%20by%2010,000%20inferences).png)

<img src="images/P78081528_HW2/Screenshot%20from%202020-11-16%2014-17-20.png" alt="Screenshot from 2020-11-16 14-17-20" style="zoom: 33%;" />

> Raw data

## Next step for improvement

We can see the red bars (time >= 3.6E-4) are all convolution layers. Therefore, the improvement may be done by parallel these layers or leverage GPU to accelerate the execution. 

## Appendix

### Code

To avoid duplicate code, I isolated the model's code into independent file and import it in train and inference code.

+ Code architecture
```bash
$ tree . -I 'env|result|__pycache__|images|*.md|*.pdf'
.
├── profile.py
└── resnet_cifar10.py
```

+ Model (`resnet_cifar10.py`)
  + Reference:
    1. https://github.com/akamaster/pytorch_resnet_cifar10/blob/master/resnet.py
    2. https://github.com/mitmul/chainer-cifar10/blob/master/models/ResNet.py

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Reference:
#   1. https://github.com/akamaster/pytorch_resnet_cifar10/blob/master/resnet.py
#   2. https://github.com/mitmul/chainer-cifar10/blob/master/models/ResNet.py
import chainer
import chainer.functions as F
import chainer.links as L


class BottleNeck(chainer.Chain):
    def __init__(self, n_in, n_out, stride=1):
        self.shortcut = stride != 1

        super(BottleNeck, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(
                n_in, n_out, ksize=3, stride=stride, pad=1, nobias=True
            )
            self.bn1 = L.BatchNormalization(n_out)
            self.conv2 = L.Convolution2D(
                n_out, n_out, ksize=3, stride=1, pad=1, nobias=True
            )
            self.bn2 = L.BatchNormalization(n_out)
            self.conv3 = L.Convolution2D(
                n_in, n_out, ksize=1, stride=stride, pad=0, nobias=True
            )
            self.bn3 = L.BatchNormalization(n_out)

    def __call__(self, x):
        h = F.relu(self.bn1(self.conv1(x)))
        h = self.bn2(self.conv2(h))
        if self.shortcut:
            h += self.bn3(self.conv3(x))
        h = F.relu(h)
        return h


class Block(chainer.ChainList):
    def __init__(self, n_in, n_out, n_bottlenecks, stride):
        super(Block, self).__init__()
        self.in_planes = n_in
        self.n_out = n_out
        strides = [stride] + [1] * (n_bottlenecks - 1)
        for stride in strides:
            self.add_link(BottleNeck(self.in_planes, n_out, stride))
            self.in_planes = n_out

    def __call__(self, x):
        for f in self:
            x = f(x)
        return x


class ResNet(chainer.Chain):
    def __init__(self, n_class=10, n_blocks=[3, 3, 3]):
        super(ResNet, self).__init__()

        with self.init_scope():
            self.conv1 = L.Convolution2D(None, 16, 3, 1, 1, nobias=True)
            self.bn2 = L.BatchNormalization(16)
            self.res3 = Block(16, 16, n_blocks[0], 1)
            self.res4 = Block(16, 32, n_blocks[1], 2)
            self.res5 = Block(32, 64, n_blocks[2], 2)
            self.fc7 = L.Linear(64, n_class)

    def __call__(self, x):
        h = F.relu(self.bn2(self.conv1(x)))
        h = self.res3(h)
        h = self.res4(h)
        h = self.res5(h)
        h = F.average_pooling_2d(h, h.shape[2:])
        h = self.fc7(h)
        return h


class ResNet20(ResNet):
    def __init__(self, n_class=10):
        super(ResNet20, self).__init__(n_class, [3, 3, 3])
```

+ Train script (`profile.py`)

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
from resnet_cifar10 import ResNet20
import argparse
import chainer
from chainer import serializers
from chainer.function_hooks import TimerHook

import numpy as np


parser = argparse.ArgumentParser(description="Chainer example: Cifar-10")
parser.add_argument(
    "--out",
    "-o",
    default="../hw1/result/5/resnet20.model",
    help="Directory to output the result",
)
parser.add_argument(
    "--unit", "-u", type=int, default=10, help="Number of output layer units"
)
# parser.add_argument('--gpu', '-g', type=int, default=0,
# help='GPU ID (negative value indicates CPU)')#Set the initial matrixformat(numpy/cupy)
if __name__ == "__main__":
    args = parser.parse_args()  # The negative device number means CPU.
    # recorder.init()
    # print('GPU: {}'.format(args.gpu))
    # print("# unit: {}".format(args.unit))
    # print("")

    # Load the dataset
    _, test = chainer.datasets.get_cifar10()
    # Load trained model

    model = ResNet20(args.unit)
    # if args.gpu >= 0:
    # chainer.cuda.get_device(args.gpu).use() # Make a specified GPU current
    # model.to_gpu() # Copy the model to the GPU
    # xp = np if args.gpu < 0 else chainer.cuda.cupy
    xp = np

    serializers.load_npz(args.out, model)

    x = chainer.Variable(xp.asarray([test[0][0]]))  # test data
    t = chainer.Variable(xp.asarray([test[0][1]]))  # labels

    trials = 10000
    num_layer = 0  # For counting the number of layers of the model
    hook = TimerHook()
    y = []
    with hook:
        for i in range(trials):
            y = model(x)  # Inference result
            if i == 0:
                num_layer = len(hook.call_history)

    i = 0
    result = {}
    for i in range(len(hook.call_history)):
        layer_i = i % num_layer
        if not layer_i in result.keys():
            result[layer_i] = {
                "type": hook.call_history[i][0],
                "time": hook.call_history[i][1],
            }
        else:
            result[layer_i]["time"] += hook.call_history[i][1]
        i += 1

    average_time = dict(
        map(lambda kv: (kv[0], (kv[1]["time"] / trials)), result.items())
    )
    # import pprint
    # pprint.pprint(hook.call_history)
    # pprint.pprint(result)
    # pprint.pprint(average_time)
    for k in result.keys():
        print(f'{k}, {result[k]["type"]}, {average_time[k]}')

    # print("The test data label:", xp.asarray([test[0][1]]))
    # print("result:", y)
    # y_top5 = y.array[0].argsort()[-5:][::-1]
    # print("result Top 1:", [y_top5[0]])
    # print("result Top 5:", y_top5)
```